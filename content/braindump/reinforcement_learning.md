+++
title = "Reinforcement Learning"
author = ["Matthew Schlegel"]
lastmod = 2022-10-27T20:20:36-06:00
slug = "reinforcement_learning"
tags = ["Reinforcement-Learning"]
draft = false
notetype = "topic"
+++

Some major categories of reinforcement learning:


## Off-policy {#off-policy}


## Successor Representations {#successor-representations}


### <span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_1">Barreto, Borsa, et al. 2018</a>) {#b01b2b}


### <span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_2">Barreto, Dabney, et al. 2018</a>) {#89e7f4}


### <span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_5">Lehnert, Tellex, and Littman 2017</a>) {#c71cbb}


### <span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_3">Borsa et al. 2018</a>) {#3de7db}


### <span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_4">Hansen et al. 2020</a>) {#dc52fb}


### <span class="org-todo todo TODO">TODO</span> (<a href="#citeproc_bib_item_6">Sherstan, Machado, and Pilarski 2018</a>) {#39da36}


## Reading {#reading}


### Textbooks: {#textbooks}

-   [Reinforcement Learning: An Introduction](http://www.incompleteideas.net/book/the-book-2nd.html) (<a href="#citeproc_bib_item_8">Sutton and Barto 2018</a>)

(<a href="#citeproc_bib_item_7">Sutton 2020</a>)



<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>Barreto, Andre, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. 2018. “Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement.” In <i>International Conference on Machine Learning</i>. PMLR.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a>Barreto, André, Will Dabney, Rémi Munos, Jonathan J. Hunt, Tom Schaul, Hado van Hasselt, and David Silver. 2018. “Successor Features for Transfer in Reinforcement Learning.” <i>arXiv:1606.05312 [Cs]</i>. <a href="https://arxiv.org/abs/1606.05312">https://arxiv.org/abs/1606.05312</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_3"></a>Borsa, Diana, André Barreto, John Quan, Daniel Mankowitz, Rémi Munos, Hado van Hasselt, David Silver, and Tom Schaul. 2018. “Universal Successor Features Approximators.” <i>arXiv:1812.07626 [Cs, Stat]</i>. <a href="https://arxiv.org/abs/1812.07626">https://arxiv.org/abs/1812.07626</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_4"></a>Hansen, Steven, Will Dabney, Andre Barreto, Tom Van de Wiele, David Warde-Farley, and Volodymyr Mnih. 2020. “Fast Task Inference with Variational Intrinsic Successor Features.” <i>arXiv:1906.05030 [Cs, Stat]</i>. <a href="https://arxiv.org/abs/1906.05030">https://arxiv.org/abs/1906.05030</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_5"></a>Lehnert, Lucas, Stefanie Tellex, and Michael L. Littman. 2017. “Advantages and Limitations of Using Successor Features for Transfer in Reinforcement Learning.” <i>arXiv:1708.00102 [Cs, Stat]</i>. <a href="https://arxiv.org/abs/1708.00102">https://arxiv.org/abs/1708.00102</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_6"></a>Sherstan, Craig, Marlos C. Machado, and Patrick M. Pilarski. 2018. “Accelerating Learning in Constructive Predictive Frameworks with the Successor Representation.” <i>arXiv:1803.09001 [Cs, Stat]</i>. <a href="https://arxiv.org/abs/1803.09001">https://arxiv.org/abs/1803.09001</a>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_7"></a>Sutton, Richard S. 2020. “John McCarthy’s Definition of Intelligence.” <i>Journal of Artificial General Intelligence</i>.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_8"></a>Sutton, Richard S., and Andrew G. Barto. 2018. <i>Reinforcement Learning: An Introduction</i>. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press.</div>
</div>
